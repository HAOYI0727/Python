# """
# ç‰¹å¾å·¥ç¨‹æ¨¡å—
# æ„å»ºæœ‰æ•ˆçš„ç‰¹å¾å¹¶è¿›è¡Œå»ºæ¨¡
# """

# import pandas as pd
# import numpy as np
# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils.data import Dataset, DataLoader, random_split
# from sklearn.preprocessing import StandardScaler, LabelEncoder
# from sklearn.metrics import mean_absolute_error, accuracy_score, classification_report
# import requests
# from bs4 import BeautifulSoup
# import time
# import os
# from pathlib import Path
# import json
# import warnings
# warnings.filterwarnings('ignore')

# class IMDbCrawler:
#     """IMDbçˆ¬è™«ç±»ï¼Œç”¨äºè·å–é¢å¤–ç”µå½±ä¿¡æ¯"""
    
#     def __init__(self, delay=1.0):
#         self.delay = delay  # è¯·æ±‚å»¶è¿Ÿï¼Œé¿å…è¢«å°IP
#         self.headers = {
#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
#         }
#         self.cache_file = Path("data/processed/imdb_cache.json")
#         self.cache = self._load_cache()
    
#     def _load_cache(self):
#         """åŠ è½½ç¼“å­˜æ•°æ®"""
#         if self.cache_file.exists():
#             with open(self.cache_file, 'r', encoding='utf-8') as f:
#                 return json.load(f)
#         return {}
    
#     def _save_cache(self):
#         """ä¿å­˜ç¼“å­˜æ•°æ®"""
#         self.cache_file.parent.mkdir(parents=True, exist_ok=True)
#         with open(self.cache_file, 'w', encoding='utf-8') as f:
#             json.dump(self.cache, f, ensure_ascii=False, indent=2)
    
#     def get_imdb_info(self, imdb_id):
#         """è·å–IMDbç”µå½±ä¿¡æ¯"""
#         if not imdb_id or imdb_id == 0:
#             return {}
        
#         # æ£€æŸ¥ç¼“å­˜
#         cache_key = str(imdb_id)
#         if cache_key in self.cache:
#             return self.cache[cache_key]
        
#         try:
#             # æ„é€ URL
#             url = f"https://www.imdb.com/title/tt{imdb_id:07d}/"
#             print(f"æ­£åœ¨çˆ¬å–: {url}")
            
#             # å‘é€è¯·æ±‚
#             response = requests.get(url, headers=self.headers, timeout=10)
#             if response.status_code != 200:
#                 print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
#                 return {}
            
#             # è§£æHTML
#             soup = BeautifulSoup(response.content, 'html.parser')
            
#             # æå–ç”µå½±ä¿¡æ¯
#             movie_info = {}
            
#             # æå–è¯„åˆ†
#             rating_element = soup.find('span', class_='sc-bde20123-1')
#             if rating_element:
#                 try:
#                     movie_info['imdb_rating'] = float(rating_element.text)
#                 except:
#                     movie_info['imdb_rating'] = None
            
#             # æå–æŠ•ç¥¨æ•°
#             votes_element = soup.find('div', class_='sc-bde20123-3')
#             if votes_element:
#                 movie_info['imdb_votes'] = votes_element.text
            
#             # æå–å‰§æƒ…ç®€ä»‹
#             summary_element = soup.find('span', class_='sc-466bb6c-0')
#             if summary_element:
#                 movie_info['summary'] = summary_element.text.strip()
            
#             # æå–å¯¼æ¼”
#             director_elements = soup.find_all('a', class_='ipc-metadata-list-item__list-content-item')
#             directors = []
#             for element in director_elements:
#                 if 'director' in element.text.lower():
#                     directors.append(element.text.strip())
#             if directors:
#                 movie_info['directors'] = directors
            
#             # ç¼“å­˜ç»“æœ
#             self.cache[cache_key] = movie_info
#             self._save_cache()
            
#             # å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
#             time.sleep(self.delay)
            
#             return movie_info
            
#         except Exception as e:
#             print(f"çˆ¬å–IMDbä¿¡æ¯å¤±è´¥ (ID: {imdb_id}): {e}")
#             return {}

# class MovieFeatureEngineer:
#     """ç”µå½±ç‰¹å¾å·¥ç¨‹ç±»"""
    
#     def __init__(self, project_path="D:/VSCodeProjects/PythonCourse"):
#         self.project_path = Path(project_path)
#         self.processed_path = self.project_path / "data" / "processed"
#         self.scaler = StandardScaler()
#         self.imdb_crawler = IMDbCrawler(delay=0.5)  # é™ä½å»¶è¿Ÿæé«˜é€Ÿåº¦
    
#     def load_processed_data(self):
#         """åŠ è½½å¤„ç†åçš„æ•°æ®"""
#         print("ğŸ“ åŠ è½½å¤„ç†åçš„æ•°æ®...")
        
#         movie_features = pd.read_csv(self.processed_path / "movie_features_clean.csv")
        
#         print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {movie_features.shape}")
#         return movie_features
    
#     def create_basic_features(self, df):
#         """åˆ›å»ºåŸºç¡€ç‰¹å¾"""
#         print("ğŸ—ï¸ åˆ›å»ºåŸºç¡€ç‰¹å¾...")
        
#         # å¤åˆ¶æ•°æ®æ¡†
#         features_df = df.copy()
        
#         # 1. æ—¶é—´ç›¸å…³ç‰¹å¾
#         current_year = 2024  # å‡è®¾å½“å‰å¹´ä»½
#         features_df['movie_age'] = current_year - features_df['year']
        
#         # 2. è¯„åˆ†ç›¸å…³ç‰¹å¾
#         features_df['rating_count_log'] = np.log1p(features_df['rating_count'])
#         features_df['has_high_rating_count'] = (features_df['rating_count'] > features_df['rating_count'].median()).astype(int)
        
#         # 3. ç±»å‹æ•°é‡ç‰¹å¾
#         genre_cols = [col for col in features_df.columns if col.startswith('genre_')]
#         features_df['genre_count'] = features_df[genre_cols].sum(axis=1)
#         features_df['has_multiple_genres'] = (features_df['genre_count'] > 1).astype(int)
        
#         # 4. è¯„åˆ†ç¨³å®šæ€§ç‰¹å¾
#         features_df['rating_stability'] = 1 / (1 + features_df['rating_std'].fillna(0))
        
#         print(f"   åˆ›å»ºäº† {len(features_df.columns) - len(df.columns)} ä¸ªæ–°ç‰¹å¾")
#         return features_df
    
#     def add_imdb_features(self, df, sample_size=None):
#         """æ·»åŠ IMDbç‰¹å¾ï¼ˆå¯é€‰ï¼‰"""
#         print("ğŸ¬ æ·»åŠ IMDbç‰¹å¾...")
        
#         # ä¸ºäº†æ¼”ç¤ºï¼Œåªå¤„ç†éƒ¨åˆ†æ•°æ®
#         if sample_size:
#             df_sample = df.head(sample_size).copy()
#         else:
#             df_sample = df.copy()
        
#         imdb_features = []
        
#         for idx, row in df_sample.iterrows():
#             imdb_id = row['imdbId']
#             features = {}
            
#             if imdb_id and imdb_id != 0:
#                 movie_info = self.imdb_crawler.get_imdb_info(imdb_id)
                
#                 # æå–æ•°å€¼å‹ç‰¹å¾
#                 features['imdb_rating'] = movie_info.get('imdb_rating', 0)
#                 features['has_imdb_rating'] = 1 if movie_info.get('imdb_rating') else 0
#                 features['summary_length'] = len(movie_info.get('summary', ''))
#                 features['director_count'] = len(movie_info.get('directors', []))
#             else:
#                 # æ²¡æœ‰IMDb IDçš„ç”µå½±
#                 features.update({
#                     'imdb_rating': 0,
#                     'has_imdb_rating': 0,
#                     'summary_length': 0,
#                     'director_count': 0
#                 })
            
#             imdb_features.append(features)
            
#             # æ˜¾ç¤ºè¿›åº¦
#             if (idx + 1) % 10 == 0:
#                 print(f"   å·²å¤„ç† {idx + 1}/{len(df_sample)} éƒ¨ç”µå½±")
        
#         # åˆå¹¶ç‰¹å¾
#         imdb_df = pd.DataFrame(imdb_features)
#         result_df = pd.concat([df_sample.reset_index(drop=True), imdb_df], axis=1)
        
#         print(f"   æ·»åŠ äº† {len(imdb_df.columns)} ä¸ªIMDbç‰¹å¾")
#         return result_df
    
#     def prepare_modeling_features(self, df, use_imdb=False, sample_size=100):
#         """å‡†å¤‡å»ºæ¨¡ç‰¹å¾"""
#         print("ğŸ”§ å‡†å¤‡å»ºæ¨¡ç‰¹å¾...")
        
#         # 1. åˆ›å»ºåŸºç¡€ç‰¹å¾
#         features_df = self.create_basic_features(df)
        
#         # 2. å¯é€‰ï¼šæ·»åŠ IMDbç‰¹å¾
#         if use_imdb:
#             features_df = self.add_imdb_features(features_df, sample_size)
        
#         # 3. é€‰æ‹©ç‰¹å¾åˆ—
#         # æ’é™¤éç‰¹å¾åˆ—
#         exclude_cols = ['movieId', 'title', 'genres', 'genres_list', 'first_rating_date', 'last_rating_date']
        
#         # é€‰æ‹©æ•°å€¼å‹ç‰¹å¾
#         numeric_features = features_df.select_dtypes(include=[np.number]).columns
#         feature_cols = [col for col in numeric_features if col not in exclude_cols]
        
#         # 4. å¤„ç†ç›®æ ‡å˜é‡
#         # é€‰é¡¹1: å›å½’ä»»åŠ¡ï¼ˆé¢„æµ‹å…·ä½“è¯„åˆ†ï¼‰
#         # é€‰é¡¹2: åˆ†ç±»ä»»åŠ¡ï¼ˆå°†è¯„åˆ†è½¬ä¸ºç±»åˆ«ï¼‰
#         features_df = self.create_target_variable(features_df)
        
#         print(f"   æœ€ç»ˆç‰¹å¾æ•°é‡: {len(feature_cols)}")
#         print(f"   ç‰¹å¾åˆ—: {feature_cols}")
        
#         return features_df, feature_cols
    
#     def create_target_variable(self, df):
#         """åˆ›å»ºç›®æ ‡å˜é‡"""
#         print("ğŸ¯ åˆ›å»ºç›®æ ‡å˜é‡...")
        
#         # æ–¹æ³•1: å›å½’ä»»åŠ¡ - ç›´æ¥ä½¿ç”¨å¹³å‡è¯„åˆ†
#         df['target_regression'] = df['avg_rating']
        
#         # æ–¹æ³•2: åˆ†ç±»ä»»åŠ¡ - å°†è¯„åˆ†åˆ†ä¸º3ä¸ªç±»åˆ«
#         # 0: ä½åˆ†(0-2.5), 1: ä¸­åˆ†(2.5-4), 2: é«˜åˆ†(4-5)
#         conditions = [
#             df['avg_rating'] <= 2.5,
#             (df['avg_rating'] > 2.5) & (df['avg_rating'] <= 4.0),
#             df['avg_rating'] > 4.0
#         ]
#         choices = [0, 1, 2]  # ä½, ä¸­, é«˜
#         df['target_classification'] = np.select(conditions, choices, default=1)
        
#         # æ–¹æ³•3: äºŒåˆ†ç±» - æ˜¯å¦é«˜äºå¹³å‡åˆ†
#         rating_mean = df['avg_rating'].mean()
#         df['target_binary'] = (df['avg_rating'] > rating_mean).astype(int)
        
#         print(f"   ç›®æ ‡å˜é‡åˆ†å¸ƒ:")
#         print(f"   - å›å½’ç›®æ ‡èŒƒå›´: {df['target_regression'].min():.2f} - {df['target_regression'].max():.2f}")
#         print(f"   - åˆ†ç±»ç›®æ ‡åˆ†å¸ƒ: {df['target_classification'].value_counts().sort_index().to_dict()}")
#         print(f"   - äºŒåˆ†ç±»åˆ†å¸ƒ: {df['target_binary'].value_counts().sort_index().to_dict()}")
        
#         return df

# class MovieDataset(Dataset):
#     """PyTorchæ•°æ®é›†ç±»"""
    
#     def __init__(self, features, targets):
#         self.features = torch.FloatTensor(features)
#         self.targets = torch.FloatTensor(targets) if targets.dtype == float else torch.LongTensor(targets)
    
#     def __len__(self):
#         return len(self.features)
    
#     def __getitem__(self, idx):
#         return self.features[idx], self.targets[idx]

# class RatingPredictor(nn.Module):
#     """ç”µå½±è¯„åˆ†é¢„æµ‹æ¨¡å‹"""
    
#     def __init__(self, input_size, output_type='regression', hidden_sizes=[128, 64, 32]):
#         super().__init__()
#         self.output_type = output_type
        
#         # æ„å»ºåŠ¨æ€ç½‘ç»œå±‚
#         layers = []
#         prev_size = input_size
        
#         for hidden_size in hidden_sizes:
#             layers.extend([
#                 nn.Linear(prev_size, hidden_size),
#                 nn.ReLU(),
#                 nn.Dropout(0.3),
#                 nn.BatchNorm1d(hidden_size)
#             ])
#             prev_size = hidden_size
        
#         self.network = nn.Sequential(*layers)
        
#         # è¾“å‡ºå±‚
#         if output_type == 'regression':
#             self.output_layer = nn.Linear(prev_size, 1)
#         elif output_type == 'binary':
#             self.output_layer = nn.Linear(prev_size, 1)
#             self.sigmoid = nn.Sigmoid()
#         else:  # multiclass
#             self.output_layer = nn.Linear(prev_size, 3)  # 3ä¸ªç±»åˆ«
    
#     def forward(self, x):
#         x = self.network(x)
#         x = self.output_layer(x)
        
#         if self.output_type == 'binary':
#             x = self.sigmoid(x)
        
#         return x.squeeze()

# class ModelTrainer:
#     """æ¨¡å‹è®­ç»ƒå™¨"""
    
#     def __init__(self, model, device='cpu'):
#         self.model = model.to(device)
#         self.device = device
        
#     def train_model(self, train_loader, val_loader, output_type='regression', 
#                    epochs=100, lr=0.001):
#         """è®­ç»ƒæ¨¡å‹"""
        
#         # é€‰æ‹©æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
#         if output_type == 'regression':
#             criterion = nn.MSELoss()
#         elif output_type == 'binary':
#             criterion = nn.BCELoss()
#         else:  # multiclass
#             criterion = nn.CrossEntropyLoss()
        
#         optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)
#         scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)
        
#         train_losses = []
#         val_losses = []
#         val_metrics = []
        
#         print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
#         for epoch in range(epochs):
#             # è®­ç»ƒé˜¶æ®µ
#             self.model.train()
#             train_loss = 0
            
#             for batch_features, batch_targets in train_loader:
#                 batch_features = batch_features.to(self.device)
#                 batch_targets = batch_targets.to(self.device)
                
#                 optimizer.zero_grad()
#                 outputs = self.model(batch_features)
                
#                 if output_type == 'multiclass':
#                     loss = criterion(outputs, batch_targets.long())
#                 else:
#                     loss = criterion(outputs, batch_targets)
                
#                 loss.backward()
#                 optimizer.step()
#                 train_loss += loss.item()
            
#             # éªŒè¯é˜¶æ®µ
#             self.model.eval()
#             val_loss = 0
#             all_predictions = []
#             all_targets = []
            
#             with torch.no_grad():
#                 for batch_features, batch_targets in val_loader:
#                     batch_features = batch_features.to(self.device)
#                     batch_targets = batch_targets.to(self.device)
                    
#                     outputs = self.model(batch_features)
                    
#                     if output_type == 'multiclass':
#                         loss = criterion(outputs, batch_targets.long())
#                         predictions = torch.argmax(outputs, dim=1)
#                     else:
#                         loss = criterion(outputs, batch_targets)
#                         predictions = outputs
                    
#                     val_loss += loss.item()
#                     all_predictions.extend(predictions.cpu().numpy())
#                     all_targets.extend(batch_targets.cpu().numpy())
            
#             # è®¡ç®—æŒ‡æ ‡
#             avg_train_loss = train_loss / len(train_loader)
#             avg_val_loss = val_loss / len(val_loader)
            
#             train_losses.append(avg_train_loss)
#             val_losses.append(avg_val_loss)
            
#             # è®¡ç®—éªŒè¯é›†æŒ‡æ ‡
#             val_metric = self.calculate_metrics(all_predictions, all_targets, output_type)
#             val_metrics.append(val_metric)
            
#             scheduler.step(avg_val_loss)
            
#             if (epoch + 1) % 20 == 0:
#                 print(f'Epoch [{epoch+1}/{epochs}], '
#                       f'Train Loss: {avg_train_loss:.4f}, '
#                       f'Val Loss: {avg_val_loss:.4f}, '
#                       f'Val Metric: {val_metric:.4f}')
        
#         return train_losses, val_losses, val_metrics
    
#     def calculate_metrics(self, predictions, targets, output_type):
#         """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
#         if output_type == 'regression':
#             return mean_absolute_error(targets, predictions)
#         else:
#             return accuracy_score(targets, predictions)
    
#     def evaluate_model(self, data_loader, output_type='regression'):
#         """è¯„ä¼°æ¨¡å‹"""
#         self.model.eval()
#         all_predictions = []
#         all_targets = []
        
#         with torch.no_grad():
#             for batch_features, batch_targets in data_loader:
#                 batch_features = batch_features.to(self.device)
#                 batch_targets = batch_targets.to(self.device)
                
#                 outputs = self.model(batch_features)
                
#                 if output_type == 'multiclass':
#                     predictions = torch.argmax(outputs, dim=1)
#                 elif output_type == 'binary':
#                     predictions = (outputs > 0.5).int()
#                 else:  # regression
#                     predictions = outputs
                
#                 all_predictions.extend(predictions.cpu().numpy())
#                 all_targets.extend(batch_targets.cpu().numpy())
        
#         return all_predictions, all_targets

# def run_experiment(features, targets, output_type='regression', experiment_num=1):
#     """è¿è¡Œå•æ¬¡å®éªŒ"""
#     print(f"\nğŸ”¬ å®éªŒ {experiment_num} - {output_type.upper()} ä»»åŠ¡")
    
#     # åˆ›å»ºæ•°æ®é›†
#     dataset = MovieDataset(features, targets)
    
#     # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
#     train_size = int(0.8 * len(dataset))
#     val_size = len(dataset) - train_size
#     train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
#     # åˆ›å»ºæ•°æ®åŠ è½½å™¨
#     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
#     val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
#     # åˆ›å»ºæ¨¡å‹
#     input_size = features.shape[1]
#     model = RatingPredictor(input_size, output_type)
    
#     # è®­ç»ƒæ¨¡å‹
#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#     trainer = ModelTrainer(model, device)
    
#     train_losses, val_losses, val_metrics = trainer.train_model(
#         train_loader, val_loader, output_type, epochs=50, lr=0.001
#     )
    
#     # æœ€ç»ˆè¯„ä¼°
#     predictions, true_targets = trainer.evaluate_model(val_loader, output_type)
    
#     if output_type == 'regression':
#         mae = mean_absolute_error(true_targets, predictions)
#         print(f"âœ… å®éªŒ {experiment_num} å®Œæˆ - MAE: {mae:.4f}")
#         return mae
#     else:
#         accuracy = accuracy_score(true_targets, predictions)
#         print(f"âœ… å®éªŒ {experiment_num} å®Œæˆ - å‡†ç¡®ç‡: {accuracy:.4f}")
#         return accuracy

# if __name__ == "__main__":
#     # è¿è¡Œç‰¹å¾å·¥ç¨‹å’Œå»ºæ¨¡
#     engineer = MovieFeatureEngineer()
#     movie_data = engineer.load_processed_data()
    
#     # å‡†å¤‡ç‰¹å¾ï¼ˆä¸ä½¿ç”¨IMDbæ•°æ®ä»¥åŠ å¿«é€Ÿåº¦ï¼‰
#     features_df, feature_cols = engineer.prepare_modeling_features(
#         movie_data, use_imdb=False
#     )
    
#     # é€‰æ‹©ç‰¹å¾å’Œç›®æ ‡
#     X = features_df[feature_cols].values
#     y_regression = features_df['target_regression'].values
#     y_classification = features_df['target_classification'].values
    
#     print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
#     print(f"å›å½’ç›®æ ‡å½¢çŠ¶: {y_regression.shape}")
#     print(f"åˆ†ç±»ç›®æ ‡å½¢çŠ¶: {y_classification.shape}")

"""
ç‰¹å¾å·¥ç¨‹æ¨¡å—
æ„å»ºæœ‰æ•ˆçš„ç‰¹å¾å¹¶è¿›è¡Œå»ºæ¨¡
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path
from torch.utils.data import Dataset, DataLoader, random_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, accuracy_score
import warnings
warnings.filterwarnings('ignore')

class MovieFeatureEngineer:
    """ç”µå½±ç‰¹å¾å·¥ç¨‹ç±»"""
    
    def __init__(self, project_path="D:/VSCodeProjects/PythonCourse"):
        self.project_path = Path(project_path)
        self.processed_path = self.project_path / "data" / "processed"
        self.scaler = StandardScaler()
    
    def load_processed_data(self):
        """åŠ è½½å¤„ç†åçš„æ•°æ®"""
        print("ğŸ“ åŠ è½½å¤„ç†åçš„æ•°æ®...")
        
        movie_features = pd.read_csv(self.processed_path / "movie_features_clean.csv")
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {movie_features.shape}")
        return movie_features
    
    def create_basic_features(self, df):
        """åˆ›å»ºåŸºç¡€ç‰¹å¾"""
        print("ğŸ—ï¸ åˆ›å»ºåŸºç¡€ç‰¹å¾...")
        
        # å¤åˆ¶æ•°æ®æ¡†
        features_df = df.copy()
        
        # 1. æ—¶é—´ç›¸å…³ç‰¹å¾
        current_year = 2024  # å‡è®¾å½“å‰å¹´ä»½
        features_df['movie_age'] = current_year - features_df['year']
        
        # 2. è¯„åˆ†ç›¸å…³ç‰¹å¾
        features_df['rating_count_log'] = np.log1p(features_df['rating_count'])
        features_df['has_high_rating_count'] = (features_df['rating_count'] > features_df['rating_count'].median()).astype(int)
        
        # 3. ç±»å‹æ•°é‡ç‰¹å¾
        genre_cols = [col for col in features_df.columns if col.startswith('genre_')]
        features_df['genre_count'] = features_df[genre_cols].sum(axis=1)
        features_df['has_multiple_genres'] = (features_df['genre_count'] > 1).astype(int)
        
        # 4. è¯„åˆ†ç¨³å®šæ€§ç‰¹å¾
        features_df['rating_stability'] = 1 / (1 + features_df['rating_std'].fillna(0))
        
        print(f"   åˆ›å»ºäº† {len(features_df.columns) - len(df.columns)} ä¸ªæ–°ç‰¹å¾")
        return features_df
    
    def add_synthetic_imdb_features(self, df):
        """æ·»åŠ åˆæˆçš„IMDbç‰¹å¾ï¼ˆåŸºäºç°æœ‰æ•°æ®ç”Ÿæˆï¼‰"""
        print("ğŸ¬ æ·»åŠ åˆæˆIMDbç‰¹å¾...")
        
        features_df = df.copy()
        
        # åŸºäºç°æœ‰è¯„åˆ†ç”Ÿæˆæ¨¡æ‹Ÿçš„IMDbè¯„åˆ†
        np.random.seed(42)  # ä¿è¯å¯é‡å¤æ€§
        
        # 1. æ¨¡æ‹ŸIMDbè¯„åˆ†ï¼ˆä¸ç°æœ‰è¯„åˆ†ç›¸å…³ä½†ç•¥æœ‰å·®å¼‚ï¼‰
        features_df['imdb_rating'] = features_df['avg_rating'] + np.random.normal(0, 0.3, len(features_df))
        features_df['imdb_rating'] = features_df['imdb_rating'].clip(1, 5)  # é™åˆ¶åœ¨1-5èŒƒå›´å†…
        
        # 2. æ¨¡æ‹Ÿæ˜¯å¦æœ‰IMDbè¯„åˆ†ï¼ˆå¤§å¤šæ•°ç”µå½±éƒ½æœ‰ï¼‰
        features_df['has_imdb_rating'] = np.random.choice([0, 1], len(features_df), p=[0.1, 0.9])
        
        # 3. æ¨¡æ‹Ÿç®€ä»‹é•¿åº¦ï¼ˆä¸ç”µå½±å¹´ä»½å’Œç±»å‹æ•°é‡ç›¸å…³ï¼‰
        base_length = 100
        features_df['summary_length'] = (
            base_length + 
            features_df['genre_count'] * 20 + 
            (2024 - features_df['year']) * 2 +
            np.random.normal(0, 50, len(features_df))
        ).astype(int).clip(50, 500)
        
        # 4. æ¨¡æ‹Ÿå¯¼æ¼”æ•°é‡
        features_df['director_count'] = np.random.choice([1, 2, 3], len(features_df), p=[0.7, 0.25, 0.05])
        
        print(f"   æ·»åŠ äº† 4 ä¸ªåˆæˆIMDbç‰¹å¾")
        return features_df
    
    def prepare_modeling_features(self, df, use_synthetic_imdb=True):
        """å‡†å¤‡å»ºæ¨¡ç‰¹å¾"""
        print("ğŸ”§ å‡†å¤‡å»ºæ¨¡ç‰¹å¾...")
        
        # 1. åˆ›å»ºåŸºç¡€ç‰¹å¾
        features_df = self.create_basic_features(df)
        
        # 2. å¯é€‰ï¼šæ·»åŠ åˆæˆIMDbç‰¹å¾
        if use_synthetic_imdb:
            features_df = self.add_synthetic_imdb_features(features_df)
        
        # 3. é€‰æ‹©ç‰¹å¾åˆ—
        # æ’é™¤éç‰¹å¾åˆ—
        exclude_cols = ['movieId', 'title', 'genres', 'genres_list', 'first_rating_date', 'last_rating_date']
        
        # é€‰æ‹©æ•°å€¼å‹ç‰¹å¾
        numeric_features = features_df.select_dtypes(include=[np.number]).columns
        feature_cols = [col for col in numeric_features if col not in exclude_cols]
        
        # 4. å¤„ç†ç›®æ ‡å˜é‡
        features_df = self.create_target_variable(features_df)
        
        print(f"   æœ€ç»ˆç‰¹å¾æ•°é‡: {len(feature_cols)}")
        print(f"   ç‰¹å¾åˆ—: {feature_cols}")
        
        return features_df, feature_cols
    
    def create_target_variable(self, df):
        """åˆ›å»ºç›®æ ‡å˜é‡"""
        print("ğŸ¯ åˆ›å»ºç›®æ ‡å˜é‡...")
        
        # æ–¹æ³•1: å›å½’ä»»åŠ¡ - ç›´æ¥ä½¿ç”¨å¹³å‡è¯„åˆ†
        df['target_regression'] = df['avg_rating']
        
        # æ–¹æ³•2: åˆ†ç±»ä»»åŠ¡ - å°†è¯„åˆ†åˆ†ä¸º3ä¸ªç±»åˆ«
        # 0: ä½åˆ†(0-2.5), 1: ä¸­åˆ†(2.5-4), 2: é«˜åˆ†(4-5)
        conditions = [
            df['avg_rating'] <= 2.5,
            (df['avg_rating'] > 2.5) & (df['avg_rating'] <= 4.0),
            df['avg_rating'] > 4.0
        ]
        choices = [0, 1, 2]  # ä½, ä¸­, é«˜
        df['target_classification'] = np.select(conditions, choices, default=1)
        
        # æ–¹æ³•3: äºŒåˆ†ç±» - æ˜¯å¦é«˜äºå¹³å‡åˆ†
        rating_mean = df['avg_rating'].mean()
        df['target_binary'] = (df['avg_rating'] > rating_mean).astype(int)
        
        print(f"   ç›®æ ‡å˜é‡åˆ†å¸ƒ:")
        print(f"   - å›å½’ç›®æ ‡èŒƒå›´: {df['target_regression'].min():.2f} - {df['target_regression'].max():.2f}")
        print(f"   - åˆ†ç±»ç›®æ ‡åˆ†å¸ƒ: {df['target_classification'].value_counts().sort_index().to_dict()}")
        print(f"   - äºŒåˆ†ç±»åˆ†å¸ƒ: {df['target_binary'].value_counts().sort_index().to_dict()}")
        
        return df

class MovieDataset(Dataset):
    """PyTorchæ•°æ®é›†ç±»"""
    
    def __init__(self, features, targets):
        self.features = torch.FloatTensor(features)
        self.targets = torch.FloatTensor(targets) if targets.dtype == float else torch.LongTensor(targets)
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return self.features[idx], self.targets[idx]

class RatingPredictor(nn.Module):
    """ç”µå½±è¯„åˆ†é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, input_size, output_type='regression', hidden_sizes=[128, 64, 32]):
        super().__init__()
        self.output_type = output_type
        
        # æ„å»ºåŠ¨æ€ç½‘ç»œå±‚
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.BatchNorm1d(hidden_size)
            ])
            prev_size = hidden_size
        
        self.network = nn.Sequential(*layers)
        
        # è¾“å‡ºå±‚
        if output_type == 'regression':
            self.output_layer = nn.Linear(prev_size, 1)
        elif output_type == 'binary':
            self.output_layer = nn.Linear(prev_size, 1)
            self.sigmoid = nn.Sigmoid()
        else:  # multiclass
            self.output_layer = nn.Linear(prev_size, 3)  # 3ä¸ªç±»åˆ«
    
    def forward(self, x):
        x = self.network(x)
        x = self.output_layer(x)
        
        if self.output_type == 'binary':
            x = self.sigmoid(x)
        
        return x.squeeze()

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model, device='cpu'):
        self.model = model.to(device)
        self.device = device
        
    def train_model(self, train_loader, val_loader, output_type='regression', 
                   epochs=100, lr=0.001):
        """è®­ç»ƒæ¨¡å‹"""
        
        # é€‰æ‹©æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        if output_type == 'regression':
            criterion = nn.MSELoss()
        elif output_type == 'binary':
            criterion = nn.BCELoss()
        else:  # multiclass
            criterion = nn.CrossEntropyLoss()
        
        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)
        
        train_losses = []
        val_losses = []
        val_metrics = []
        
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0
            
            for batch_features, batch_targets in train_loader:
                batch_features = batch_features.to(self.device)
                batch_targets = batch_targets.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(batch_features)
                
                if output_type == 'multiclass':
                    loss = criterion(outputs, batch_targets.long())
                else:
                    loss = criterion(outputs, batch_targets)
                
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            
            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_loss = 0
            all_predictions = []
            all_targets = []
            
            with torch.no_grad():
                for batch_features, batch_targets in val_loader:
                    batch_features = batch_features.to(self.device)
                    batch_targets = batch_targets.to(self.device)
                    
                    outputs = self.model(batch_features)
                    
                    if output_type == 'multiclass':
                        loss = criterion(outputs, batch_targets.long())
                        predictions = torch.argmax(outputs, dim=1)
                    else:
                        loss = criterion(outputs, batch_targets)
                        predictions = outputs
                    
                    val_loss += loss.item()
                    all_predictions.extend(predictions.cpu().numpy())
                    all_targets.extend(batch_targets.cpu().numpy())
            
            # è®¡ç®—æŒ‡æ ‡
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            
            # è®¡ç®—éªŒè¯é›†æŒ‡æ ‡
            val_metric = self.calculate_metrics(all_predictions, all_targets, output_type)
            val_metrics.append(val_metric)
            
            scheduler.step(avg_val_loss)
            
            if (epoch + 1) % 20 == 0:
                print(f'Epoch [{epoch+1}/{epochs}], '
                      f'Train Loss: {avg_train_loss:.4f}, '
                      f'Val Loss: {avg_val_loss:.4f}, '
                      f'Val Metric: {val_metric:.4f}')
        
        return train_losses, val_losses, val_metrics
    
    def calculate_metrics(self, predictions, targets, output_type):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        if output_type == 'regression':
            return mean_absolute_error(targets, predictions)
        else:
            return accuracy_score(targets, predictions)
    
    def evaluate_model(self, data_loader, output_type='regression'):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for batch_features, batch_targets in data_loader:
                batch_features = batch_features.to(self.device)
                batch_targets = batch_targets.to(self.device)
                
                outputs = self.model(batch_features)
                
                if output_type == 'multiclass':
                    predictions = torch.argmax(outputs, dim=1)
                elif output_type == 'binary':
                    predictions = (outputs > 0.5).int()
                else:  # regression
                    predictions = outputs
                
                all_predictions.extend(predictions.cpu().numpy())
                all_targets.extend(batch_targets.cpu().numpy())
        
        return all_predictions, all_targets

def run_experiment(features, targets, output_type='regression', experiment_num=1):
    """è¿è¡Œå•æ¬¡å®éªŒ"""
    print(f"\nğŸ”¬ å®éªŒ {experiment_num} - {output_type.upper()} ä»»åŠ¡")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = MovieDataset(features, targets)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    input_size = features.shape[1]
    model = RatingPredictor(input_size, output_type)
    
    # è®­ç»ƒæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    trainer = ModelTrainer(model, device)
    
    train_losses, val_losses, val_metrics = trainer.train_model(
        train_loader, val_loader, output_type, epochs=50, lr=0.001
    )
    
    # æœ€ç»ˆè¯„ä¼°
    predictions, true_targets = trainer.evaluate_model(val_loader, output_type)
    
    if output_type == 'regression':
        mae = mean_absolute_error(true_targets, predictions)
        print(f"âœ… å®éªŒ {experiment_num} å®Œæˆ - MAE: {mae:.4f}")
        return mae
    else:
        accuracy = accuracy_score(true_targets, predictions)
        print(f"âœ… å®éªŒ {experiment_num} å®Œæˆ - å‡†ç¡®ç‡: {accuracy:.4f}")
        return accuracy

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ ç”µå½±è¯„åˆ†é¢„æµ‹æ¨¡å‹è®­ç»ƒå¼€å§‹...")
    
    # è¿è¡Œç‰¹å¾å·¥ç¨‹å’Œå»ºæ¨¡
    engineer = MovieFeatureEngineer()
    
    try:
        movie_data = engineer.load_processed_data()
    except FileNotFoundError:
        print("âŒ æ‰¾ä¸åˆ°å¤„ç†åçš„æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†æ¨¡å—")
        return
    
    # å‡†å¤‡ç‰¹å¾ï¼ˆä½¿ç”¨åˆæˆIMDbæ•°æ®ï¼‰
    features_df, feature_cols = engineer.prepare_modeling_features(
        movie_data, use_synthetic_imdb=True
    )
    
    # é€‰æ‹©ç‰¹å¾å’Œç›®æ ‡
    X = features_df[feature_cols].values
    y_regression = features_df['target_regression'].values
    y_classification = features_df['target_classification'].values
    y_binary = features_df['target_binary'].values
    
    print(f"\nğŸ“Š æ•°æ®å‡†å¤‡å®Œæˆ:")
    print(f"   ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
    print(f"   å›å½’ç›®æ ‡å½¢çŠ¶: {y_regression.shape}")
    print(f"   åˆ†ç±»ç›®æ ‡å½¢çŠ¶: {y_classification.shape}")
    print(f"   äºŒåˆ†ç±»ç›®æ ‡å½¢çŠ¶: {y_binary.shape}")
    
    # è¿è¡Œä¸åŒä»»åŠ¡çš„å®éªŒ
    results = {}
    
    # å®éªŒ1: å›å½’ä»»åŠ¡
    results['regression'] = run_experiment(X, y_regression, 'regression', 1)
    
    # å®éªŒ2: å¤šåˆ†ç±»ä»»åŠ¡
    results['classification'] = run_experiment(X, y_classification, 'multiclass', 2)
    
    # å®éªŒ3: äºŒåˆ†ç±»ä»»åŠ¡
    results['binary'] = run_experiment(X, y_binary, 'binary', 3)
    
    print(f"\nğŸ¯ æ‰€æœ‰å®éªŒå®Œæˆ!")
    print(f"   å›å½’ä»»åŠ¡ MAE: {results['regression']:.4f}")
    print(f"   å¤šåˆ†ç±»å‡†ç¡®ç‡: {results['classification']:.4f}")
    print(f"   äºŒåˆ†ç±»å‡†ç¡®ç‡: {results['binary']:.4f}")

if __name__ == "__main__":
    main()