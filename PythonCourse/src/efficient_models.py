"""
é«˜æ•ˆæœºå™¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹å®ç°
ä½¿ç”¨sklearnå’ŒPyTorch
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier
from sklearn.svm import SVR, SVC
from sklearn.linear_model import Ridge, Lasso, LogisticRegression
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.metrics import mean_absolute_error, accuracy_score, classification_report
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

class EfficientModelTrainer:
    """é«˜æ•ˆæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.results = {}
        self.scaler = StandardScaler()
    
    def prepare_data(self, features_df, feature_cols, target_type='regression', test_size=0.2):
        """å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
        X = features_df[feature_cols].values
        
        if target_type == 'regression':
            y = features_df['target_regression'].values
        elif target_type == 'multiclass':
            y = features_df['target_classification'].values
        else:  # binary
            y = features_df['target_binary'].values
        
        # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y if target_type != 'regression' else None
        )
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        return X_train_scaled, X_test_scaled, y_train, y_test
    
    def train_sklearn_models(self, X_train, X_test, y_train, y_test, target_type='regression'):
        """è®­ç»ƒsklearnæ¨¡å‹"""
        print("ğŸ¤– è®­ç»ƒScikit-learnæ¨¡å‹...")
        
        models = {}
        predictions = {}
        scores = {}
        
        # å®šä¹‰æ¨¡å‹é…ç½®
        if target_type == 'regression':
            model_configs = {
                'RandomForest': RandomForestRegressor(
                    n_estimators=200, max_depth=15, min_samples_split=5, 
                    random_state=42, n_jobs=-1
                ),
                'GradientBoosting': GradientBoostingRegressor(
                    n_estimators=200, max_depth=6, learning_rate=0.1, 
                    random_state=42
                ),
                'Ridge': Ridge(alpha=1.0, random_state=42),
                'Lasso': Lasso(alpha=0.1, random_state=42),
                'KNN': KNeighborsRegressor(n_neighbors=5, weights='distance'),
                'SVM': SVR(kernel='rbf', C=1.0, gamma='scale')
            }
        else:
            model_configs = {
                'RandomForest': RandomForestClassifier(
                    n_estimators=200, max_depth=15, min_samples_split=5, 
                    random_state=42, n_jobs=-1
                ),
                'GradientBoosting': GradientBoostingClassifier(
                    n_estimators=200, max_depth=6, learning_rate=0.1, 
                    random_state=42
                ),
                'LogisticRegression': LogisticRegression(
                    C=1.0, random_state=42, max_iter=1000, n_jobs=-1
                ),
                'KNN': KNeighborsClassifier(n_neighbors=5, weights='distance'),
                'SVM': SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
            }
        
        # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
        for name, model in model_configs.items():
            try:
                print(f"   è®­ç»ƒ {name}...")
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                
                models[name] = model
                predictions[name] = y_pred
                
                # è®¡ç®—åˆ†æ•°
                if target_type == 'regression':
                    score = mean_absolute_error(y_test, y_pred)
                    scores[name] = score
                    print(f"     {name} MAE: {score:.4f}")
                else:
                    score = accuracy_score(y_test, y_pred)
                    scores[name] = score
                    print(f"     {name} å‡†ç¡®ç‡: {score:.4f}")
                    
            except Exception as e:
                print(f"     {name} è®­ç»ƒå¤±è´¥: {e}")
        
        return models, predictions, scores
    
    def train_deep_learning_model(self, X_train, X_test, y_train, y_test, target_type='regression'):
        """è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹"""
        print("ğŸ§  è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹...")
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        X_train_tensor = torch.FloatTensor(X_train)
        X_test_tensor = torch.FloatTensor(X_test)
        
        if target_type == 'regression':
            y_train_tensor = torch.FloatTensor(y_train)
            y_test_tensor = torch.FloatTensor(y_test)
        else:
            y_train_tensor = torch.LongTensor(y_train)
            y_test_tensor = torch.LongTensor(y_test)
        
        # åˆ›å»ºæ•°æ®é›†
        class MovieDataset(Dataset):
            def __init__(self, features, targets):
                self.features = features
                self.targets = targets
            
            def __len__(self):
                return len(self.features)
            
            def __getitem__(self, idx):
                return self.features[idx], self.targets[idx]
        
        train_dataset = MovieDataset(X_train_tensor, y_train_tensor)
        test_dataset = MovieDataset(X_test_tensor, y_test_tensor)
        
        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
        
        # å®šä¹‰æ”¹è¿›çš„ç¥ç»ç½‘ç»œ
        class AdvancedNet(nn.Module):
            def __init__(self, input_size, output_type='regression'):
                super().__init__()
                self.output_type = output_type
                
                self.network = nn.Sequential(
                    nn.Linear(input_size, 256),
                    nn.BatchNorm1d(256),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    
                    nn.Linear(256, 128),
                    nn.BatchNorm1d(128),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    
                    nn.Linear(128, 64),
                    nn.BatchNorm1d(64),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    
                    nn.Linear(64, 32),
                    nn.BatchNorm1d(32),
                    nn.ReLU(),
                )
                
                if output_type == 'regression':
                    self.output_layer = nn.Linear(32, 1)
                elif output_type == 'binary':
                    self.output_layer = nn.Sequential(
                        nn.Linear(32, 1),
                        nn.Sigmoid()
                    )
                else:  # multiclass
                    self.output_layer = nn.Sequential(
                        nn.Linear(32, 3),
                        nn.LogSoftmax(dim=1)
                    )
            
            def forward(self, x):
                x = self.network(x)
                return self.output_layer(x).squeeze()
        
        # è®­ç»ƒé…ç½®
        input_size = X_train.shape[1]
        model = AdvancedNet(input_size, target_type)
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        
        if target_type == 'regression':
            criterion = nn.MSELoss()
        elif target_type == 'binary':
            criterion = nn.BCELoss()
        else:
            criterion = nn.NLLLoss()
        
        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)
        
        # è®­ç»ƒå¾ªç¯
        train_losses = []
        val_losses = []
        best_val_loss = float('inf')
        patience = 15
        counter = 0
        
        for epoch in range(100):
            # è®­ç»ƒ
            model.train()
            train_loss = 0
            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                
                optimizer.zero_grad()
                outputs = model(batch_X)
                
                if target_type == 'multiclass':
                    loss = criterion(outputs, batch_y)
                else:
                    loss = criterion(outputs, batch_y)
                
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            
            # éªŒè¯
            model.eval()
            val_loss = 0
            with torch.no_grad():
                for batch_X, batch_y in test_loader:
                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                    outputs = model(batch_X)
                    
                    if target_type == 'multiclass':
                        loss = criterion(outputs, batch_y)
                    else:
                        loss = criterion(outputs, batch_y)
                    
                    val_loss += loss.item()
            
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(test_loader)
            
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            
            scheduler.step(avg_val_loss)
            
            # æ—©åœ
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                counter = 0
                best_model_state = model.state_dict().copy()
            else:
                counter += 1
            
            if counter >= patience:
                print(f"    æ—©åœåœ¨ç¬¬ {epoch+1} è½®")
                break
            
            if (epoch + 1) % 20 == 0:
                print(f"    è½®æ¬¡ {epoch+1}, è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        model.load_state_dict(best_model_state)
        
        # æœ€ç»ˆé¢„æµ‹
        model.eval()
        with torch.no_grad():
            test_predictions = []
            for batch_X, _ in test_loader:
                batch_X = batch_X.to(device)
                outputs = model(batch_X)
                
                if target_type == 'multiclass':
                    _, predicted = torch.max(outputs, 1)
                    test_predictions.extend(predicted.cpu().numpy())
                elif target_type == 'binary':
                    predicted = (outputs > 0.5).int()
                    test_predictions.extend(predicted.cpu().numpy())
                else:
                    test_predictions.extend(outputs.cpu().numpy())
        
        # è®¡ç®—åˆ†æ•°
        if target_type == 'regression':
            score = mean_absolute_error(y_test, test_predictions)
            print(f"    æ·±åº¦å­¦ä¹ æ¨¡å‹ MAE: {score:.4f}")
        else:
            score = accuracy_score(y_test, test_predictions)
            print(f"    æ·±åº¦å­¦ä¹ æ¨¡å‹ å‡†ç¡®ç‡: {score:.4f}")
        
        return model, test_predictions, score, train_losses, val_losses
    
    def run_complete_experiment(self, features_df, feature_cols, target_type='regression'):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        print(f"\nğŸ¯ å¼€å§‹{target_type.upper()}ä»»åŠ¡å®éªŒ")
        print("=" * 60)
        
        # å‡†å¤‡æ•°æ®
        X_train, X_test, y_train, y_test = self.prepare_data(
            features_df, feature_cols, target_type
        )
        
        # è®­ç»ƒsklearnæ¨¡å‹
        sklearn_models, sklearn_predictions, sklearn_scores = self.train_sklearn_models(
            X_train, X_test, y_train, y_test, target_type
        )
        
        # è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹
        dl_model, dl_predictions, dl_score, train_losses, val_losses = self.train_deep_learning_model(
            X_train, X_test, y_train, y_test, target_type
        )
        
        # åˆå¹¶ç»“æœ
        all_scores = sklearn_scores.copy()
        all_scores['DeepLearning'] = dl_score
        
        all_predictions = sklearn_predictions.copy()
        all_predictions['DeepLearning'] = dl_predictions
        
        # æ˜¾ç¤ºç»“æœæ’å
        self.display_ranking(all_scores, target_type)
        
        return {
            'scores': all_scores,
            'predictions': all_predictions,
            'sklearn_models': sklearn_models,
            'dl_model': dl_model,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'X_test': X_test,
            'y_test': y_test
        }
    
    def display_ranking(self, scores, target_type):
        """æ˜¾ç¤ºæ¨¡å‹æ’å"""
        print(f"\nğŸ† {target_type.upper()}ä»»åŠ¡æ¨¡å‹æ’å:")
        print("-" * 40)
        
        if target_type == 'regression':
            # MAEè¶Šä½è¶Šå¥½
            sorted_scores = sorted(scores.items(), key=lambda x: x[1])
            for i, (model, score) in enumerate(sorted_scores, 1):
                print(f"   {i}. {model}: MAE = {score:.4f}")
        else:
            # å‡†ç¡®ç‡è¶Šé«˜è¶Šå¥½
            sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
            for i, (model, score) in enumerate(sorted_scores, 1):
                print(f"   {i}. {model}: å‡†ç¡®ç‡ = {score:.4f}")

def run_multiple_experiments(features_df, feature_cols, num_experiments=3):
    """è¿è¡Œå¤šæ¬¡å®éªŒ"""
    print("ğŸ”¬ å¼€å§‹å¤šæ¬¡å®éªŒéªŒè¯")
    print("=" * 60)
    
    all_regression_results = []
    all_classification_results = []
    
    for exp in range(num_experiments):
        print(f"\nğŸ† å®éªŒ {exp+1}/{num_experiments}")
        print("-" * 40)
        
        # è®¾ç½®ä¸åŒçš„éšæœºç§å­
        np.random.seed(42 + exp)
        torch.manual_seed(42 + exp)
        
        trainer = EfficientModelTrainer()
        
        # å›å½’ä»»åŠ¡
        reg_results = trainer.run_complete_experiment(
            features_df, feature_cols, 'regression'
        )
        all_regression_results.append(reg_results)
        
        # åˆ†ç±»ä»»åŠ¡
        cls_results = trainer.run_complete_experiment(
            features_df, feature_cols, 'multiclass'
        )
        all_classification_results.append(cls_results)
    
    return all_regression_results, all_classification_results