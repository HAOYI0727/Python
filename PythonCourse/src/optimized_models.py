"""
ä¼˜åŒ–ç‰ˆé«˜æ•ˆæ¨¡å‹å®ç°
åœ¨åŸæœ‰åŸºç¡€ä¸Šå¢åŠ æ›´å¤šæ¨¡å‹å’Œæ”¹è¿›çš„æ·±åº¦å­¦ä¹ ç½‘ç»œ
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from sklearn.ensemble import (RandomForestRegressor, RandomForestClassifier,
                             GradientBoostingRegressor, GradientBoostingClassifier,
                             ExtraTreesRegressor, ExtraTreesClassifier,
                             AdaBoostRegressor, AdaBoostClassifier)
from sklearn.svm import SVR, SVC
from sklearn.linear_model import Ridge, Lasso, ElasticNet, LogisticRegression
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.metrics import mean_absolute_error, accuracy_score, classification_report
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

class OptimizedModelTrainer:
    """ä¼˜åŒ–ç‰ˆæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.results = {}
        self.scaler = StandardScaler()
    
    def prepare_data(self, features_df, feature_cols, target_type='regression', test_size=0.2):
        """å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
        X = features_df[feature_cols].values
        
        if target_type == 'regression':
            y = features_df['target_regression'].values
        elif target_type == 'multiclass':
            y = features_df['target_classification'].values
        else:  # binary
            y = features_df['target_binary'].values
        
        # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, 
            stratify=y if target_type != 'regression' else None
        )
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        return X_train_scaled, X_test_scaled, y_train, y_test
    
    def train_advanced_sklearn_models(self, X_train, X_test, y_train, y_test, target_type='regression'):
        """è®­ç»ƒæ›´å¤šsklearnæ¨¡å‹"""
        print("ğŸ¤– è®­ç»ƒé«˜çº§Scikit-learnæ¨¡å‹...")
        
        models = {}
        predictions = {}
        scores = {}
        
        # æ‰©å±•çš„æ¨¡å‹é…ç½®
        if target_type == 'regression':
            model_configs = {
                # æ ‘æ¨¡å‹å®¶æ—
                'RandomForest': RandomForestRegressor(
                    n_estimators=200, max_depth=20, min_samples_split=5, 
                    random_state=42, n_jobs=-1
                ),
                'ExtraTrees': ExtraTreesRegressor(
                    n_estimators=200, max_depth=20, random_state=42, n_jobs=-1
                ),
                'GradientBoosting': GradientBoostingRegressor(
                    n_estimators=200, max_depth=6, learning_rate=0.1, 
                    subsample=0.8, random_state=42
                ),
                'AdaBoost': AdaBoostRegressor(
                    n_estimators=100, learning_rate=0.1, random_state=42
                ),
                'DecisionTree': DecisionTreeRegressor(
                    max_depth=15, random_state=42
                ),
                
                # çº¿æ€§æ¨¡å‹
                'Ridge': Ridge(alpha=1.0, random_state=42),
                'Lasso': Lasso(alpha=0.1, random_state=42),
                'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42),
                
                # å…¶ä»–æ¨¡å‹
                'KNN': KNeighborsRegressor(n_neighbors=7, weights='distance'),
                'SVM': SVR(kernel='rbf', C=1.0, gamma='scale'),
                'MLP': MLPRegressor(
                    hidden_layer_sizes=(100, 50), activation='relu',
                    learning_rate_init=0.001, max_iter=500, random_state=42
                )
            }
        else:
            model_configs = {
                # æ ‘æ¨¡å‹å®¶æ—
                'RandomForest': RandomForestClassifier(
                    n_estimators=200, max_depth=20, min_samples_split=5, 
                    random_state=42, n_jobs=-1
                ),
                'ExtraTrees': ExtraTreesClassifier(
                    n_estimators=200, max_depth=20, random_state=42, n_jobs=-1
                ),
                'GradientBoosting': GradientBoostingClassifier(
                    n_estimators=200, max_depth=6, learning_rate=0.1, 
                    subsample=0.8, random_state=42
                ),
                'AdaBoost': AdaBoostClassifier(
                    n_estimators=100, learning_rate=0.1, random_state=42
                ),
                'DecisionTree': DecisionTreeClassifier(
                    max_depth=15, random_state=42
                ),
                
                # çº¿æ€§æ¨¡å‹
                'LogisticRegression': LogisticRegression(
                    C=1.0, random_state=42, max_iter=1000, n_jobs=-1
                ),
                
                # å…¶ä»–æ¨¡å‹
                'KNN': KNeighborsClassifier(n_neighbors=7, weights='distance'),
                'SVM': SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42),
                'MLP': MLPClassifier(
                    hidden_layer_sizes=(100, 50), activation='relu',
                    learning_rate_init=0.001, max_iter=500, random_state=42
                )
            }
        
        # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
        for name, model in model_configs.items():
            try:
                print(f"   è®­ç»ƒ {name}...")
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                
                models[name] = model
                predictions[name] = y_pred
                
                # è®¡ç®—åˆ†æ•°
                if target_type == 'regression':
                    score = mean_absolute_error(y_test, y_pred)
                    scores[name] = score
                    print(f"     {name} MAE: {score:.4f}")
                else:
                    score = accuracy_score(y_test, y_pred)
                    scores[name] = score
                    print(f"     {name} å‡†ç¡®ç‡: {score:.4f}")
                    
            except Exception as e:
                print(f"     {name} è®­ç»ƒå¤±è´¥: {e}")
        
        return models, predictions, scores
    
    def train_advanced_neural_network(self, X_train, X_test, y_train, y_test, target_type='regression'):
        """è®­ç»ƒæ”¹è¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹"""
        print("ğŸ§  è®­ç»ƒæ”¹è¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹...")
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        X_train_tensor = torch.FloatTensor(X_train)
        X_test_tensor = torch.FloatTensor(X_test)
        
        if target_type == 'regression':
            y_train_tensor = torch.FloatTensor(y_train)
            y_test_tensor = torch.FloatTensor(y_test)
        else:
            y_train_tensor = torch.LongTensor(y_train)
            y_test_tensor = torch.LongTensor(y_test)
        
        # åˆ›å»ºæ•°æ®é›†
        class AdvancedMovieDataset(Dataset):
            def __init__(self, features, targets):
                self.features = features
                self.targets = targets
            
            def __len__(self):
                return len(self.features)
            
            def __getitem__(self, idx):
                return self.features[idx], self.targets[idx]
        
        train_dataset = AdvancedMovieDataset(X_train_tensor, y_train_tensor)
        test_dataset = AdvancedMovieDataset(X_test_tensor, y_test_tensor)
        
        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
        
        # å®šä¹‰å¤šç§æ”¹è¿›çš„ç¥ç»ç½‘ç»œæ¶æ„
        class ResidualBlock(nn.Module):
            """æ®‹å·®å—"""
            def __init__(self, input_size, output_size):
                super().__init__()
                self.linear1 = nn.Linear(input_size, output_size)
                self.bn1 = nn.BatchNorm1d(output_size)
                self.linear2 = nn.Linear(output_size, output_size)
                self.bn2 = nn.BatchNorm1d(output_size)
                self.dropout = nn.Dropout(0.3)
                self.shortcut = nn.Linear(input_size, output_size) if input_size != output_size else nn.Identity()
            
            def forward(self, x):
                residual = self.shortcut(x)
                out = self.linear1(x)
                out = self.bn1(out)
                out = torch.relu(out)
                out = self.dropout(out)
                out = self.linear2(out)
                out = self.bn2(out)
                out += residual
                out = torch.relu(out)
                return out
        
        class AdvancedNetV1(nn.Module):
            """æ·±åº¦æ®‹å·®ç½‘ç»œ"""
            def __init__(self, input_size, output_type='regression'):
                super().__init__()
                self.output_type = output_type
                
                self.input_layer = nn.Sequential(
                    nn.Linear(input_size, 256),
                    nn.BatchNorm1d(256),
                    nn.ReLU(),
                    nn.Dropout(0.4)
                )
                
                self.res_blocks = nn.Sequential(
                    ResidualBlock(256, 256),
                    ResidualBlock(256, 128),
                    ResidualBlock(128, 64),
                )
                
                if output_type == 'regression':
                    self.output_layer = nn.Linear(64, 1)
                elif output_type == 'binary':
                    self.output_layer = nn.Sequential(
                        nn.Linear(64, 1),
                        nn.Sigmoid()
                    )
                else:  # multiclass
                    self.output_layer = nn.Sequential(
                        nn.Linear(64, 32),
                        nn.ReLU(),
                        nn.Linear(32, 3),
                        nn.Softmax(dim=1)
                    )
            
            def forward(self, x):
                x = self.input_layer(x)
                x = self.res_blocks(x)
                return self.output_layer(x).squeeze()
        
        class AdvancedNetV2(nn.Module):
            """å®½ç½‘ç»œæ¶æ„"""
            def __init__(self, input_size, output_type='regression'):
                super().__init__()
                self.output_type = output_type
                
                self.network = nn.Sequential(
                    nn.Linear(input_size, 512),
                    nn.BatchNorm1d(512),
                    nn.ReLU(),
                    nn.Dropout(0.4),
                    
                    nn.Linear(512, 256),
                    nn.BatchNorm1d(256),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    
                    nn.Linear(256, 128),
                    nn.BatchNorm1d(128),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    
                    nn.Linear(128, 64),
                    nn.BatchNorm1d(64),
                    nn.ReLU(),
                )
                
                if output_type == 'regression':
                    self.output_layer = nn.Linear(64, 1)
                elif output_type == 'binary':
                    self.output_layer = nn.Sequential(
                        nn.Linear(64, 1),
                        nn.Sigmoid()
                    )
                else:
                    self.output_layer = nn.Sequential(
                        nn.Linear(64, 3),
                        nn.LogSoftmax(dim=1)
                    )
            
            def forward(self, x):
                x = self.network(x)
                return self.output_layer(x).squeeze()
        
        # è®­ç»ƒå¤šä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹
        input_size = X_train.shape[1]
        dl_models = {
            'DeepResNet': AdvancedNetV1(input_size, target_type),
            'DeepWideNet': AdvancedNetV2(input_size, target_type)
        }
        
        dl_results = {}
        
        for model_name, model in dl_models.items():
            print(f"   è®­ç»ƒ {model_name}...")
            
            # è®­ç»ƒé…ç½®
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            model.to(device)
            
            if target_type == 'regression':
                criterion = nn.MSELoss()
            elif target_type == 'binary':
                criterion = nn.BCELoss()
            else:
                criterion = nn.NLLLoss()
            
            optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)
            
            # è®­ç»ƒå¾ªç¯
            model.train()
            best_val_loss = float('inf')
            patience = 10
            counter = 0
            
            for epoch in range(100):
                # è®­ç»ƒ
                train_loss = 0
                for batch_X, batch_y in train_loader:
                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                    
                    optimizer.zero_grad()
                    outputs = model(batch_X)
                    
                    if target_type == 'multiclass':
                        loss = criterion(outputs, batch_y)
                    else:
                        loss = criterion(outputs, batch_y)
                    
                    loss.backward()
                    optimizer.step()
                    train_loss += loss.item()
                
                scheduler.step()
                
                # éªŒè¯
                model.eval()
                val_loss = 0
                with torch.no_grad():
                    for batch_X, batch_y in test_loader:
                        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                        outputs = model(batch_X)
                        
                        if target_type == 'multiclass':
                            loss = criterion(outputs, batch_y)
                        else:
                            loss = criterion(outputs, batch_y)
                        
                        val_loss += loss.item()
                
                # æ—©åœ
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    counter = 0
                    best_model_state = model.state_dict().copy()
                else:
                    counter += 1
                
                if counter >= patience:
                    break
            
            # åŠ è½½æœ€ä½³æ¨¡å‹
            model.load_state_dict(best_model_state)
            
            # æœ€ç»ˆé¢„æµ‹
            model.eval()
            with torch.no_grad():
                test_predictions = []
                for batch_X, _ in test_loader:
                    batch_X = batch_X.to(device)
                    outputs = model(batch_X)
                    
                    if target_type == 'multiclass':
                        _, predicted = torch.max(outputs, 1)
                        test_predictions.extend(predicted.cpu().numpy())
                    elif target_type == 'binary':
                        predicted = (outputs > 0.5).int()
                        test_predictions.extend(predicted.cpu().numpy())
                    else:
                        test_predictions.extend(outputs.cpu().numpy())
            
            # è®¡ç®—åˆ†æ•°
            if target_type == 'regression':
                score = mean_absolute_error(y_test, test_predictions)
                print(f"     {model_name} MAE: {score:.4f}")
            else:
                score = accuracy_score(y_test, test_predictions)
                print(f"     {model_name} å‡†ç¡®ç‡: {score:.4f}")
            
            dl_results[model_name] = {
                'model': model,
                'predictions': test_predictions,
                'score': score
            }
        
        # é€‰æ‹©æœ€ä½³æ·±åº¦å­¦ä¹ æ¨¡å‹
        best_dl_model = min(dl_results.items(), key=lambda x: x[1]['score'])[0] if target_type == 'regression' else \
                       max(dl_results.items(), key=lambda x: x[1]['score'])[0]
        
        print(f"   æœ€ä½³æ·±åº¦å­¦ä¹ æ¨¡å‹: {best_dl_model}")
        
        return dl_results[best_dl_model]['model'], dl_results[best_dl_model]['predictions'], \
               dl_results[best_dl_model]['score'], dl_results
    
    def run_optimized_experiment(self, features_df, feature_cols, target_type='regression'):
        """è¿è¡Œä¼˜åŒ–ç‰ˆå®éªŒ"""
        print(f"\nğŸ¯ å¼€å§‹ä¼˜åŒ–ç‰ˆ{target_type.upper()}ä»»åŠ¡å®éªŒ")
        print("=" * 60)
        
        # å‡†å¤‡æ•°æ®
        X_train, X_test, y_train, y_test = self.prepare_data(
            features_df, feature_cols, target_type
        )
        
        # è®­ç»ƒæ›´å¤šsklearnæ¨¡å‹
        sklearn_models, sklearn_predictions, sklearn_scores = self.train_advanced_sklearn_models(
            X_train, X_test, y_train, y_test, target_type
        )
        
        # è®­ç»ƒæ”¹è¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹
        dl_model, dl_predictions, dl_score, all_dl_results = self.train_advanced_neural_network(
            X_train, X_test, y_train, y_test, target_type
        )
        
        # åˆå¹¶ç»“æœ
        all_scores = sklearn_scores.copy()
        all_scores['DeepLearning'] = dl_score
        
        all_predictions = sklearn_predictions.copy()
        all_predictions['DeepLearning'] = dl_predictions
        
        # æ˜¾ç¤ºç»“æœæ’å
        self.display_ranking(all_scores, target_type)
        
        return {
            'scores': all_scores,
            'predictions': all_predictions,
            'sklearn_models': sklearn_models,
            'dl_model': dl_model,
            'all_dl_results': all_dl_results,
            'X_test': X_test,
            'y_test': y_test
        }
    
    def display_ranking(self, scores, target_type):
        """æ˜¾ç¤ºæ¨¡å‹æ’å"""
        print(f"\nğŸ† {target_type.upper()}ä»»åŠ¡æ¨¡å‹æ’å:")
        print("-" * 40)
        
        if target_type == 'regression':
            # MAEè¶Šä½è¶Šå¥½
            sorted_scores = sorted(scores.items(), key=lambda x: x[1])
            for i, (model, score) in enumerate(sorted_scores, 1):
                print(f"   {i}. {model}: MAE = {score:.4f}")
        else:
            # å‡†ç¡®ç‡è¶Šé«˜è¶Šå¥½
            sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
            for i, (model, score) in enumerate(sorted_scores, 1):
                print(f"   {i}. {model}: å‡†ç¡®ç‡ = {score:.4f}")

def run_optimized_multiple_experiments(features_df, feature_cols, num_experiments=3):
    """è¿è¡Œå¤šæ¬¡ä¼˜åŒ–ç‰ˆå®éªŒ"""
    print("ğŸ”¬ å¼€å§‹ä¼˜åŒ–ç‰ˆå¤šæ¬¡å®éªŒéªŒè¯")
    print("=" * 60)
    
    all_regression_results = []
    all_classification_results = []
    
    for exp in range(num_experiments):
        print(f"\nğŸ† å®éªŒ {exp+1}/{num_experiments}")
        print("-" * 40)
        
        # è®¾ç½®ä¸åŒçš„éšæœºç§å­
        np.random.seed(42 + exp)
        torch.manual_seed(42 + exp)
        
        trainer = OptimizedModelTrainer()
        
        # å›å½’ä»»åŠ¡
        reg_results = trainer.run_optimized_experiment(
            features_df, feature_cols, 'regression'
        )
        all_regression_results.append(reg_results)
        
        # åˆ†ç±»ä»»åŠ¡
        cls_results = trainer.run_optimized_experiment(
            features_df, feature_cols, 'multiclass'
        )
        all_classification_results.append(cls_results)
    
    return all_regression_results, all_classification_results